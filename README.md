# 日志分析器

这是一个用于分析Web服务访问日志的Python脚本，能够统计总请求数、平均响应时间、HTTP状态码分布和最繁忙时段。

## 功能特性

- 统计日志文件中的总请求次数
- 计算所有请求的平均响应时长（毫秒）
- 统计每种HTTP状态码出现的次数
- 确定一天中请求最繁忙的小时（0-23）

## 如何运行

### 环境要求
- Python 3.6+

### 运行指令

```bash
# 直接运行
python log_analyzer.py /path/to/access.log

# 或添加执行权限后运行
chmod +x log_analyzer.py
./log_analyzer.py /path/to/access.log
```

### 示例输出

```json
{
  "total_requests": 1500,
  "average_response_time_ms": 185.5,
  "status_code_counts": {
    "200": 1350,
    "404": 100,
    "500": 50
  },
  "busiest_hour": 19
}
```

## 实现思路

1. **命令行参数处理**：使用`sys.argv`获取日志文件路径参数
2. **逐行读取日志**：使用`open()`函数按行读取，避免一次性加载大文件
3. **JSON解析**：使用Python内置的`json`模块解析每行日志
4. **错误处理**：捕获并处理JSON解析错误和格式错误，确保程序健壮性
5. **统计指标计算**：
   - 总请求数：简单计数器
   - 平均响应时间：累加所有响应时间后除以总请求数
   - 状态码统计：使用字典存储各状态码出现次数
   - 最繁忙时段：使用字典存储每小时请求数，然后找出最大值
6. **数据结构选择**：使用`defaultdict`存储状态码和小时统计，因为哈希表结构提供O(1)的查找和插入效率

## 可以优化的地方

当前实现在处理小到中等大小的文件时表现良好，但如果日志文件非常大（如10GB），可能会遇到以下问题：

1. **单线程处理限制**：处理速度受限于单核CPU性能
2. **内存使用**：虽然逐行读取避免了加载整个文件，但所有统计数据仍存储在内存中

**改进方案**：
- 使用多线程/多进程并行处理：将文件分块，由多个工作线程同时处理不同部分
- 使用流式处理：对于超大文件，可以使用更高效的数据结构（如HyperLogLog）进行近似统计
- 引入外部存储：对于极端大的文件，可以将中间结果定期写入磁盘数据库
- 使用更高效的JSON解析库，如`ujson`或`orjson`
- 添加进度指示器，对于大文件处理提供用户反馈

## 性能考虑

当前实现的时间复杂度为O(n)，其中n是日志行数，空间复杂度为O(k)，其中k是唯一状态码和小时的数量。对于大多数实际应用场景，这是足够的。
